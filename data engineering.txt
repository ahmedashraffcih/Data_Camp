DATA ENGINEER DELIVER :
- the correct data 
- in the right form 
- to the right people
- as efficiently as possilbe


the five vs

- volume 
- velocity
- value 
- veracity
- variety


data science workflow
- data collection and storage
- data prepration
- exploration and visualization
- experimentation and prediction

data engineer tools 

databases
mysql - postgresql
Processing
spark - hive
Scheduling- make sure these jobs run in timely fashion and the right order
apache airflow 

cloud
---------
Storage
- aws s3
- azure blob storage
-google cloud storage

computation
---------
- aws ec2
- azure vm
- google compute engine

Databases
---------
- aws rds
- azure sql database
 - google cloud sql
_________________________________
Databases
----------

SQL
- Tables
- DB Scheme
- Relational Db
= mysql - postgreSQL

NoSQL
- NON-RELATIONAL
- Structured or unstructured
- key-value stores
- document db
= redis - mongo




------------
Hadoop 
- collection of open source packages for big data
- map redudce is part of it 
- hdfs is part of it 


PySpark
-  pythom interface for the spark framework
- use dataframe abstraction


Hive

- intially use hadoop map reduce 
- is built from the need to use structured queries for parellel processing 



DAGs
- Directed acyclic graph	
- set of noed
- directed edges
- no cycles

tools for dag
-----
linx cron
spotify luigi
apache airflow - created at airbnb



--------------------------
 JSON : Javascript Object Notation


Applications Databases
- Transactions
- inserts or changes
- oltp
- row-oriented

Analytical Databases
- Olap
- Column Orinted


JDBC helps spark to connect relational databases


MPP Databases
- Massively parallel processing database
= Amazon redshift
= Azure SQL data warehouse
- Google Bigquery






---------------------------
Introduction to CSVs
============================


Loading csv with pandas
++++++++++++++++

x = pd.read_csv("filename.csv")

x = pd.read_csv("filename.csv",sep="\t")
sep for delimiter
x.head(4) = check first 4 rows

-> limiting coulmns : usecols
# accept a list of cols name or cols numbers, or function to filter

-> skip rows: skiprows
-> limiting rows: nrows
#skiprows:  accept a list of cols name or cols numbers, or function to filter
# set Header=none so pandas knows there are no coulmn names
x = pd.read_csv("filename.csv",nrows=1000)

Assign Column names
++++++++++++++++
col_names= list(tax_data_first1000)
tax_data= pd.read_csv("filename",nrows=500,skiprows=1000,header=none,names=col_names)


specifying data types : dtype
++++++++++++++++

x = pd.read_csv("filename.csv",dtype={"zipcode":str")


customizing missing values : na_values
++++++++++++++++
x = pd.read_csv("filename.csv",na_values={"zipcode":0")


#set error_bad_lines=false to skip unparsable records
#set warn_bad_lines=true to see messages when records are skipped

---------------------------------------------------------------------

Introduction to spreadsheets
============================

x = pd.read_excel("filename.xlsx")
-> limiting rows: nrows
-> skip rows: skiprows
-> limiting coulmns : usecols
#sheet_name
#sheets are 0-indexed
to load all the sheets-> sheet_name = none, so we need to create data frame to hold all sheets then iterate through it


specifying data types : dtype
++++++++++++++++

x = pd.read_excel("filename.xlsx",dtype={"zipcode":str")


Setting Custom True/False Values
++++++++++++++++
true_values=["Yes"]
false_values=["NO"]


#datetime loaded as objects
parse_date -> accept list of cols names or numbers or sublist or dict 
use pd.to_datetime() if parse_date won't work
to_datetime(dataframe,col,format)


%Y = 4 digit
%m = 0 padded
%d = 0 padded
%H = 24-HOUR CLOCK
%M = 0 padded
%S = 0 padded


03292016  21:25:12
format_str= "%m%d%Y %H:%M:%S"



---------------------------------------------------------------------

Introduction to databases
============================
1- SQL alchemy (library) -> from sqlalchemy import create_engine

#handle db connections : create_engine()
#SQLite URL Format : sqlite:///filename.db

x = pd.read_sql(query,engine)
# quere could be : sql statement or table name 

---------------------------------------------------------------------

Introduction to JSON
============================
x = pd.read_json(string path,orient="split")
// cant read dicts


import requests
# requests.get(url_string) to get data from url

#params : dict of parameters
#headers : dict,used to provide user authentication
- headers={"Authorization": "Beareer {}" .format(api_key)}

# response.json() returns a dict so we need to load it into DF




Working with nested JSONs
++++++++++++++++
pandas.io.json   submodule has tools to read & write json

- json_normalize() => takes dict or list of dicts, returns df, choose diff seperator (sep)

from pandas.io.json import json_normalize


++++++++++++++++
Combining multiple datasets
++++++++++++++++
- Append -> df.append(df2)

- Merge() like join in sql

# x.merge(y,left_on="x",right_on="y"


========================================================
========================================================
========================================================
Writing Efficient Python Code
- Efficient is fast run time and small memory usage
- to time our code :%timeit\
- set no. of run -r and no. of loops -n
- save output -o



#Code Profiling 
==============================
- detailed stats on frequency and duration of function calls
- line by line analysis
- package used : line_profiler
= %load_ext line_profiler
= %lprun -f function function(x,y) > magic command


Code profiling for memory usage
==============================
sys.getsizeof(object)


#Code Profiling : memory
==============================

- package used : memory_profiler
= %load_ext memory_profiler
= %mprun -f function function(x,y) > magic command


The collections module
==============================
- built in module
- specialized container datatypes
*Notable : 
   - Namedtuple : tuble subclass with name fields
   - deque : list-like with fast append and pops
   - Counter : dict for counting hashable objects
   - Orderdict : dict that retains order of entries
   - Defaultdict : dict that calls factory function to supply missing values
# from collections import counter
counter(x) rather than loop through dict and count 

The Itertools module
==============================
- built in module
- Functional tools for creating and using iterators
*Notable : 
   - infinte iterators : count, cycle , repeat
   - finite iterators : accumlate, chain, zip_longest, ..etc
   - Combining generators : product, premutations, combinations
# from Itertools import combinations
combinations(x)


set theory 
==============================
- Branch of mathematics applied to collection of objects
- built in set has methods: 
   - intersection()
   - difference()
   - symmetric_difference()
   - union()
- using in operator


Intro to pandas DataFrame iteration
==============================
- Iterating with .iterrows()
for i,row in x_df.iterrows() -> skipping the inxed step

itertuples() 

.apply() method : takes function and applies it to a DF
   - Must specify axis : 0 Columns and 1 for rows


   ---------------
   BATCH SCRIPTING

   - pwd : print working directory ' tells you where you are '

   - ls : listing contents of your current directory
      - -R see everything underneath a directory
      - -F that prints a / after the name of every directory and * after the name of every runnable program
   
   ***The shell decides if a path is absolute or relative by looking at its first character: 
   *** If it begins with /, it is absolute. If it does not begin with /, it is relative.

   - cp:copy
   - mv: move
   - rm: remove
   - rmdir: remove directory only when its empty
   - mkdir directory_name: to create a new (empty) directory.
   - cat: view file content 
   - less: one page is displayed at a time; you can press spacebar to page down or type q to quit.
   -- :n next file     :p   previeos file    :q to quit

   - head: prints the first few lines of a file     -n 3 first 3 rows
   - tail:prints the last few lines of a file     -n +3 display all but first 3 lines 
      head and tail let you select rows from a text file

   -cut: select coulmns 
      -b: select only these bytes
      -c: select only these characters
      -d: use delm instead of tab for field delimiter
      -f: select only these fields also print any line that contains no delimiter, unless -s option is specified 
      -s: only delimted 

      head and tail select rows, cut selects columns, and grep selects lines according to what they contain.

   - grep: takes a piece of text followed by one or more filenames and prints all of the lines in those files that contain that text
      -c: print a count of matching lines rather than the lines themselves
      -h: do not print the names of files when searching multiple files
      -i: ignore case (e.g., treat "Regression" and "regression" as matches)
      -l: print the names of files that contain matches, not the matches
      -n: print line numbers for matching lines
      -v: invert the match, i.e., only show lines that don't match

   - paste: combine data files instead of cutting them up.
   -  > redirection for data
   -  | : pipe -> instead of copying data to another file we use it
   - (tap): auto complete in shell
   - wc: (short for "word count") prints the number of characters, words, and lines in a file.   (c,w,l)
   - man: command (short for "manual")
   - history: shows u what u have done    
   - Wild cards 
      - *
      - ? matches a single character, so 201?.txt will match 2017.txt or 2018.txt, but not 2017-01.txt
      - [...] matches any one of the characters inside the square brackets, so 201[78].txt matches 2017.txt or 2018.txt, but not 2016.txt.
      - {...} matches any of the comma-separated patterns inside the curly brackets, so {*.txt, *.csv} matches any file whose name 
              ends with .txt or .csv, but not files whose names end with .pdf.
   
   - sort
      -n , -r : sort numerically and reverse the order of its output,
      -b : ignore leading blanks
      -f : if it fold case
   - uniq: remove duplicate lines -c to count

   *shell stores information in variables, called environment variables
   - shell variable : training=seasonal/summer.csv
   - echo:  find a variable's value 

   - for loop structure : for filetype in gif jpg png; do echo $filetype; done
         - for …variable… in …list… ; do …body… ; done
   -nano text editor
      Ctrl + K: delete a line.
      Ctrl + U: un-delete a line.
      Ctrl + O: save the file ('O' stands for 'output'). You will also need to press Enter to confirm the filename!
      Ctrl + X: exit the editor.